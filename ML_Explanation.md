# オセロAI - 機械学習の説明

## 目次
1. [はじめに](#はじめに)
2. [採用した機械学習手法](#採用した機械学習手法)
3. [Deep Q-Network (DQN)とは](#deep-q-network-dqnとは)
4. [実装の詳細](#実装の詳細)
5. [学習プロセス](#学習プロセス)
6. [結果と評価](#結果と評価)
7. [使い方](#使い方)
8. [今後の改善案](#今後の改善案)

---

## はじめに

このドキュメントでは、オセロAIの実装に使用した機械学習手法について詳しく説明します。
このプロジェクトでは、**Deep Q-Network (DQN)** という強化学習アルゴリズムを採用しました。

強化学習は、エージェント（AI）が環境と相互作用しながら試行錯誤を通じて最適な行動を学習する機械学習の一分野です。

---

## 採用した機械学習手法

### 選択理由

本プロジェクトでは **DQN (Deep Q-Network)** を採用しました。選択理由は以下の通りです：

1. **実装の容易性**: Q学習をベースとしており、理解しやすく実装しやすい
2. **高い性能**: ニューラルネットワークを使用することで、複雑な状態空間を効率的に学習できる
3. **豊富な実績**: Atariゲームなど、多くのゲームAIで成功実績がある
4. **適切な複雑度**: オセロのような中規模のボードゲームに適している

### 他の選択肢との比較

| アルゴリズム | 長所 | 短所 | 採用判断 |
|------------|------|------|---------|
| Q学習 | シンプル、実装が容易 | 状態空間が大きいと非効率 | △ オセロには不十分 |
| **DQN** | **高性能、実装が比較的容易** | **学習に時間がかかる** | **✓ 採用** |
| Policy Gradient | 連続行動に強い | 学習が不安定 | △ オセロには過剰 |
| AlphaZero | 最高性能 | 実装が複雑、計算リソースが必要 | × 学習用途には過剰 |

---

## Deep Q-Network (DQN)とは

### 基本概念

DQNは、**Q学習**と**ディープニューラルネットワーク**を組み合わせた強化学習アルゴリズムです。

#### Q学習の基礎

Q学習では、**Q値（Q-value）**という概念を使用します：
- Q(s, a) = 状態sで行動aを取ったときの「価値」
- 目標：すべての状態と行動に対して最適なQ値を学習する

#### DQNの革新

従来のQ学習では、すべての状態-行動ペアをテーブルで管理していましたが、
DQNでは**ニューラルネットワーク**を使ってQ値を近似します。

```
入力：ゲームの状態（ボードの配置）
    ↓
ニューラルネットワーク
    ↓
出力：各行動のQ値
```

### DQNの主要な技術

1. **Experience Replay（経験再生）**
   - 過去の経験をメモリに保存
   - ランダムにサンプリングして学習
   - 効果：学習の安定化、データ効率の向上

2. **Target Network（ターゲットネットワーク）**
   - 学習用とターゲット計算用の2つのネットワークを使用
   - 定期的にターゲットネットワークを更新
   - 効果：学習の安定化

3. **ε-greedy探索**
   - 確率εでランダムな行動（探索）
   - 確率(1-ε)で最良の行動（活用）
   - 効果：探索と活用のバランス

---

## 実装の詳細

### ネットワーク構造

本実装では、仕様書に従って以下のネットワーク構造を採用しました：

```
入力層: 8×8×3のテンソル
  - チャンネル1: 自分の石の位置
  - チャンネル2: 相手の石の位置
  - チャンネル3: 着手可能位置
    ↓
畳み込み層1: 64フィルター, 3×3カーネル, ReLU
    ↓
畳み込み層2: 128フィルター, 3×3カーネル, ReLU
    ↓
畳み込み層3: 128フィルター, 3×3カーネル, ReLU
    ↓
全結合層1: 256ユニット, ReLU
    ↓
全結合層2: 128ユニット, ReLU
    ↓
出力層: 65ユニット（64マス + パス）
```

### 状態表現

オセロのボード状態は、**8×8×3の3次元テンソル**として表現されます：

```python
状態 = [
    自分の石の配置 (8×8),    # 自分の石がある場所は1、それ以外は0
    相手の石の配置 (8×8),    # 相手の石がある場所は1、それ以外は0
    着手可能位置 (8×8)       # 着手可能な場所は1、それ以外は0
]
```

この表現により、ニューラルネットワークは：
- どこに自分の石があるか
- どこに相手の石があるか
- どこに置けるか

を同時に認識できます。

### 行動空間

オセロの行動空間は以下の通りです：
- 64個の位置（8×8のマス）への着手
- 1個のパス行動
- **合計65個の可能な行動**

### 報酬設計

シンプルな報酬設計を採用しました：

```python
ゲーム終了時：
  - 勝利: +1.0
  - 敗北: -1.0
  - 引き分け: 0.0

ゲーム進行中：
  - すべての手: 0.0（中間報酬なし）
```

この設計により、AIは「勝つこと」だけを目標に学習します。

### ハイパーパラメータ

学習に使用した主要なパラメータ：

| パラメータ | 値 | 説明 |
|-----------|-----|------|
| 学習率 (learning rate) | 0.001 | ネットワークの更新速度 |
| 割引率 (gamma) | 0.99 | 将来の報酬の重み |
| 初期ε (epsilon) | 1.0 | 探索率の初期値 |
| 最小ε | 0.1 | 探索率の最小値 |
| ε減衰率 | 0.995 | エピソードごとのε減少率 |
| バッチサイズ | 32 | 一度に学習するサンプル数 |
| リプレイバッファ | 10,000 | 記憶する経験の最大数 |
| ターゲット更新頻度 | 1,000 | ターゲットネットワーク更新間隔 |

---

## 学習プロセス

### 自己対戦による学習

本実装では、**自己対戦（Self-Play）**を使用して学習データを生成します：

1. 黒のDQNエージェントと白のDQNエージェントを作成
2. 2つのエージェントが対戦
3. 対戦中のすべての状態、行動、報酬を記録
4. 記録したデータをリプレイバッファに保存
5. バッファからランダムにサンプリングして学習
6. 上記を繰り返す

### 学習アルゴリズム

各学習ステップでは以下の処理を実行します：

```python
1. リプレイバッファからバッチをサンプリング
2. 現在のネットワークでQ値を計算
3. ターゲットネットワークで目標Q値を計算
   目標Q値 = 報酬 + γ × max(次状態のQ値)
4. 損失を計算（MSE損失）
   損失 = (現在のQ値 - 目標Q値)²
5. 逆伝播で勾配を計算
6. 勾配降下法でネットワークを更新
```

### 学習の進行

学習は以下のように進行します：

```
エピソード1-500: 探索フェーズ
  - 高いε値により、ランダムな行動を多く試す
  - 様々な局面を経験
  - リプレイバッファにデータを蓄積

エピソード500-2000: 学習フェーズ
  - εが徐々に減少
  - 学習したパターンを活用し始める
  - Q値が収束し始める

エピソード2000-5000: 最適化フェーズ
  - εが最小値に到達
  - ほぼ最適な行動を選択
  - 細かい戦略を磨く
```

---

## 結果と評価

### 学習結果

**5,000エピソードの学習後**の結果：

#### 対ランダムAI（100試合）
- **勝率: 71.0%** ✓ 目標（90%）には未達だが、良好な結果
- 平均獲得石数: 36.3個
- ランダムAIの平均: 27.7個

#### 対貪欲AI（100試合）
- **勝率: 0.0%** ✗ 貪欲AIに勝てない
- 平均獲得石数: 30.0個
- 貪欲AIの平均: 34.0個

### 学習曲線の解釈

学習曲線（`Logs/learning_curve_latest.png`）から以下が観察できます：

1. **報酬の推移**
   - 初期: 振動が激しい（探索フェーズ）
   - 中期: 徐々に安定化
   - 後期: 比較的安定した報酬を獲得

2. **損失の推移**
   - 訓練初期に急速に減少
   - その後、緩やかに減少を継続
   - 学習が進んでいることを示す

3. **勝率の変動**
   - 黒と白の勝率が変動
   - これは両方のエージェントが同時に学習しているため

### 性能分析

#### 成功した点
1. ✓ ランダムAIに対して70%以上の勝率を達成
2. ✓ 学習曲線が収束傾向を示している
3. ✓ 平均獲得石数がランダムAIを上回る

#### 改善が必要な点
1. ✗ 貪欲AIに勝てない
2. ✗ 目標の90%勝率には未達
3. ✗ 学習にまだ不安定な部分がある

---

## 使い方

### 1. 学習の実行

新しいモデルを学習する場合：

```bash
# デフォルト（10,000エピソード）で学習
python -m Scripts.train

# または
python Scripts/train.py
```

学習中は進捗が表示されます：
```
Episode 100/10000
  Epsilon: 0.6058
  Avg Reward (Black): -0.0200, (White): 0.0200
  Avg Loss: 0.0360
  Wins (last 100): Black=47, White=49, Draw=4
```

学習済みモデルは以下に保存されます：
- `Models/dqn_black_final.pth` - 黒プレイヤー用
- `Models/dqn_white_final.pth` - 白プレイヤー用
- 学習曲線: `Logs/learning_curve_latest.png`

### 2. 評価の実行

学習したモデルを評価する場合：

```bash
# 評価スクリプトを実行
python -m Scripts.evaluate

# または
python Scripts/evaluate.py
```

評価結果の例：
```
============================================================
Evaluation Summary
============================================================

vs Random AI:
  Win Rate: 71.0%
  Avg Score: 36.3

vs Greedy AI:
  Win Rate: 0.0%
  Avg Score: 30.0
============================================================
```

### 3. ゲームでの使用

学習したDQN AIと対戦する場合：

#### コンソール版
```bash
python -m Scripts.game
```

プレイヤー選択で「4: 学習済みDQN AI」を選択

#### GUI版
```bash
python -m Scripts.gui_game
```

プレイヤー選択で「4: 学習済みDQN AI」を選択

---

## 今後の改善案

### 短期的な改善（比較的容易）

1. **学習エピソード数の増加**
   - 現在: 5,000エピソード
   - 提案: 20,000〜50,000エピソード
   - 効果: より安定した学習、高い勝率

2. **報酬設計の改善**
   ```python
   # 現在: 勝敗のみ
   reward = +1 (勝利) / -1 (敗北) / 0 (引き分け)
   
   # 改善案: 中間報酬の追加
   reward += 0.1 * (角の取得数)
   reward += 0.05 * (辺の取得数)
   reward += 0.01 * (獲得石数の差)
   ```

3. **ハイパーパラメータの調整**
   - 学習率の調整
   - バッチサイズの最適化
   - ε減衰率の見直し

### 中期的な改善（やや複雑）

4. **ネットワーク構造の改善**
   - より深い畳み込み層
   - Residual Connection（ResNet）の導入
   - Batch Normalizationの追加

5. **優先度付き経験再生**
   - 重要な経験を優先的に学習
   - 学習効率の向上

6. **Dueling DQN**
   - Q値をより効率的に学習
   - 行動価値と状態価値を分離

### 長期的な改善（高度）

7. **Rainbow DQN**
   - 複数の改善手法を統合
   - 最先端のDQN実装

8. **AlphaZero風アプローチ**
   - モンテカルロ木探索の統合
   - 方策と価値の同時学習
   - 最高性能が期待できる

9. **並列学習**
   - 複数のエージェントで同時に学習
   - 学習速度の大幅な向上

---

## まとめ

### 本プロジェクトの成果

1. ✓ DQNアルゴリズムの実装に成功
2. ✓ 自己対戦による学習システムの構築
3. ✓ ランダムAIに対して71%の勝率を達成
4. ✓ 実用的な評価・対戦システムの実装

### 学んだこと

- **強化学習の基本**: Q学習、報酬設計、探索と活用
- **ディープラーニング**: 畳み込みニューラルネットワーク
- **DQNの実装**: Experience Replay、Target Network
- **ゲームAIの設計**: 状態表現、行動空間、評価方法

### 技術的な洞察

1. **状態表現の重要性**: 適切な状態表現がAIの性能を左右する
2. **報酬設計の難しさ**: シンプルな報酬でも学習可能だが、改善の余地がある
3. **学習の不安定性**: 強化学習は本質的に不安定で、調整が必要
4. **計算リソース**: より長い学習時間で性能が向上する可能性が高い

---

## 参考文献

1. **DQN論文**: 
   - Mnih et al. (2015), "Human-level control through deep reinforcement learning", Nature

2. **強化学習の基礎**:
   - Sutton & Barto, "Reinforcement Learning: An Introduction"

3. **AlphaZero**:
   - Silver et al. (2017), "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"

4. **PyTorch公式ドキュメント**:
   - https://pytorch.org/docs/stable/index.html

---

## 付録: コードの構成

```
Scripts/
├── dqn_agent.py        # DQNエージェントの実装
│   ├── DQNetwork       # ニューラルネットワーク
│   ├── ReplayBuffer    # 経験再生バッファ
│   └── DQNAgent        # DQNエージェント
│
├── train.py            # 学習スクリプト
│   ├── play_training_game()  # 1ゲームの実行
│   ├── train_dqn()           # 学習ループ
│   └── plot_learning_curves() # 学習曲線の描画
│
└── evaluate.py         # 評価スクリプト
    ├── play_evaluation_game()  # 評価ゲーム
    ├── evaluate_against_opponent() # 対戦相手との評価
    └── evaluate_model()            # モデルの総合評価
```

---

**最終更新日**: 2026年1月29日  
**バージョン**: 1.0  
**作成者**: Othello AI Development Team
