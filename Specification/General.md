# [強化学習練習]オセロAI作成のための仕様書

## はじめに
このドキュメントは、強化学習を用いてオセロAIを作成するための仕様書です。オセロのルール、AIの設計方針、学習方法、評価基準などについて詳細に記述します。

## 開発順番
[1] オセロが遊べるように、プログラムを完成させる
[2] 強化学習を用いてAIを作成する
[3] 作成したAIの評価を行う

## オセロのルール

### 基本ルール
- ボードサイズ: 8×8のマス
- 初期配置: 中央4マスに白黒2個ずつ交互に配置
  ```
  ・・・・・・・・
  ・・・・・・・・
  ・・・・・・・・
  ・・・○●・・・
  ・・・●○・・・
  ・・・・・・・・
  ・・・・・・・・
  ・・・・・・・・
  ```
- 先手: 黒石（●）
- 後手: 白石（○）

### 着手ルール
1. プレイヤーは自分の色の石を1つずつ置く
2. 相手の石を自分の石で挟んだ場合、挟まれた石はすべて自分の色に変わる
3. 石を置くことで相手の石を1つ以上挟める場所にのみ着手可能
4. 着手可能な場所がない場合はパス
5. 両プレイヤーが連続でパスした場合、ゲーム終了
6. ボードがすべて埋まった場合もゲーム終了
7. 終了時に石の数が多い方が勝利

### 挟む方向
以下の8方向すべてで石を挟むことができる：
- 上、下、左、右
- 右上、右下、左上、左下

## プログラム設計（フェーズ1）

### 必要な機能
1. **ボード管理**
   - 8×8のボード状態の表現
   - 石の配置情報の保持
   - ボード表示機能

2. **ゲームロジック**
   - 着手可能位置の判定
   - 石の反転処理
   - ゲーム終了判定
   - 勝敗判定

3. **プレイヤーインターフェース**
   - 手動入力での対戦機能
   - 着手位置の入力
   - ゲーム状態の表示

4. **ゲーム進行管理**
   - ターン管理
   - パス処理
   - ゲームフロー制御

### データ構造
- **ボード表現**: 8×8の2次元配列
  - 0: 空マス
  - 1: 黒石
  - 2: 白石
- **座標**: (行, 列)の形式、0-7のインデックス

## AI設計（フェーズ2）

### 強化学習アルゴリズムの選択
以下のアルゴリズムから選択・実装する：
1. **Q学習**
   - シンプルで実装しやすい
   - 状態-行動価値関数を学習
   
2. **Deep Q-Network (DQN)**
   - ニューラルネットワークを用いた価値関数の近似
   - 大規模な状態空間に対応可能
   
3. **Policy Gradient（方策勾配法）**
   - 方策を直接学習
   - より高度な戦略の獲得が可能

4. **AlphaZero風アプローチ（発展課題）**
   - モンテカルロ木探索とニューラルネットワークの組み合わせ
   - 高性能だが実装難易度が高い

### 状態表現
- **入力**: ボードの現在状態（8×8×3のテンソル）
  - チャンネル1: 自分の石の位置
  - チャンネル2: 相手の石の位置
  - チャンネル3: 着手可能位置
  
### 行動空間
- 64個の位置（8×8）への着手 + パス
- 合計65の行動

### 報酬設計
基本的な報酬設定：
- ゲーム勝利: +1
- ゲーム敗北: -1
- 引き分け: 0
- 中間報酬（オプション）:
  - 相手の石を多く取る: +0.01 × 獲得石数
  - 角を取る: +0.1
  - 辺を取る: +0.05

### ネットワーク構造（DQN使用時）
```
入力層: 8×8×3
↓
畳み込み層1: 64フィルター, 3×3カーネル, ReLU
↓
畳み込み層2: 128フィルター, 3×3カーネル, ReLU
↓
畳み込み層3: 128フィルター, 3×3カーネル, ReLU
↓
全結合層1: 256ユニット, ReLU
↓
全結合層2: 128ユニット, ReLU
↓
出力層: 65ユニット（行動価値）
```

### 学習手法
1. **自己対戦による学習**
   - AIが自分自身と対戦してデータを生成
   - Experience Replay（経験再生）の利用
   
2. **学習パラメータ**
   - 学習率: 0.001（初期値）
   - 割引率γ: 0.99
   - ε-greedy法のε: 1.0 → 0.1（線形減衰）
   - バッチサイズ: 32
   - リプレイバッファサイズ: 10000
   - ターゲットネットワーク更新頻度: 1000ステップ

3. **学習スケジュール**
   - 総エピソード数: 10000～100000
   - 評価頻度: 1000エピソードごと
   - モデル保存頻度: 1000エピソードごと

### 実装言語・ライブラリ
- **言語**: Python 3.8以上
- **機械学習フレームワーク**: 
  - PyTorch または TensorFlow/Keras
- **その他ライブラリ**:
  - NumPy: 数値計算
  - Matplotlib: 学習曲線の可視化
  - pickle: モデルの保存・読み込み

## 評価基準（フェーズ3）

### 評価方法
1. **ランダムAI対戦**
   - 完全ランダムに着手するAIとの勝率
   - 目標: 勝率90%以上

2. **貪欲AI対戦**
   - 毎回最も多く石を取れる手を選ぶAIとの勝率
   - 目標: 勝率70%以上

3. **自己対戦の進化**
   - 学習初期のモデルvs学習後期のモデル
   - 学習の進歩を確認

4. **人間対戦**
   - 人間プレイヤーとの対戦
   - 主観的な強さの評価

### 評価指標
- **勝率**: 100試合中の勝利数
- **平均獲得石数**: 終了時の平均石数
- **学習曲線**: エピソード数vs報酬の推移
- **収束時間**: 安定した性能に到達するまでのエピソード数

### 性能改善の方向性
学習が不十分な場合の改善策：
1. ネットワーク構造の変更（層数、ユニット数の調整）
2. ハイパーパラメータの調整
3. 報酬設計の見直し
4. 学習データの増強
5. より高度なアルゴリズムの採用（AlphaZero等）

## プロジェクト管理

### ディレクトリ構成
```
Othello-AI/
├── Scripts/
│   ├── game.py           # ゲームロジック
│   ├── board.py          # ボード管理
│   ├── player.py         # プレイヤー（人間・AI）
│   ├── ai_agent.py       # AIエージェント
│   ├── train.py          # 学習スクリプト
│   ├── evaluate.py       # 評価スクリプト
│   └── utils.py          # ユーティリティ関数
├── Models/               # 学習済みモデル保存先
├── Logs/                 # 学習ログ・結果保存先
├── Specification/        # 仕様書
└── README.md
```

### 開発マイルストーン
- **Week 1-2**: フェーズ1（ゲーム実装）
- **Week 3-4**: フェーズ2（AI実装・学習環境構築）
- **Week 5-6**: フェーズ2（学習実行・パラメータ調整）
- **Week 7**: フェーズ3（評価・改善）
- **Week 8**: ドキュメント整備・発表準備

## 参考文献・リソース
- オセロのルール: [World Othello Federation](https://www.worldothello.org/)
- 強化学習の基礎: Sutton & Barto "Reinforcement Learning: An Introduction"
- DQN論文: "Human-level control through deep reinforcement learning" (Mnih et al., 2015)
- AlphaZero論文: "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm" (Silver et al., 2017)

## 付録

### 拡張機能（オプション）
- GUIの実装（Pygame等）
- オンライン対戦機能
- 棋譜の保存・再生機能
- 思考過程の可視化
- 複数のAIモデルのトーナメント

### トラブルシューティング
- **学習が収束しない**: 学習率の調整、報酬設計の見直し
- **過学習**: ドロップアウトの追加、正則化の強化
- **メモリ不足**: バッチサイズの削減、リプレイバッファサイズの削減
- **学習時間が長い**: GPUの利用、並列化の検討

